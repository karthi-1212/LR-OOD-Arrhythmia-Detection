{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load ECG data\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Deep Learning/Project/100.csv')\n",
    "\n",
    "# Drop unnecessary columns and extract ECG signal (MLII lead)\n",
    "df = df.drop(['Unnamed: 0', 'time_ms'], axis=1)\n",
    "ecg_signal = df['MLII'].values\n",
    "\n",
    "# Normalize the ECG signal\n",
    "ecg_signal = (ecg_signal - np.mean(ecg_signal)) / np.std(ecg_signal)\n",
    "\n",
    "# Plot the ECG signal (First 1000 samples)\n",
    "plt.plot(ecg_signal[:1000])\n",
    "plt.title('Normalized ECG Signal (First 1000 samples)')\n",
    "plt.show()\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data = ecg_signal[:len(ecg_signal)//2]\n",
    "test_data = ecg_signal[len(ecg_signal)//2:]\n",
    "\n",
    "# Further split train data into training and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32).unsqueeze(0).unsqueeze(2)\n",
    "val_data_tensor = torch.tensor(val_data, dtype=torch.float32).unsqueeze(0).unsqueeze(2)\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32).unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "# Define LSTM Model with optimizations\n",
    "class OptimizedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super(OptimizedLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "model = OptimizedLSTMModel()\n",
    "background_model = OptimizedLSTMModel()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "background_optimizer = optim.Adam(background_model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler to reduce LR if validation loss plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)\n",
    "\n",
    "# Training with early stopping and reduced epochs\n",
    "epochs = 50\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass on training data\n",
    "    output = model(train_data_tensor)\n",
    "    train_loss = criterion(output, train_data_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(val_data_tensor)\n",
    "        val_loss = criterion(val_output, val_data_tensor)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Background model training with reduced complexity\n",
    "for epoch in range(epochs):\n",
    "    background_model.train()\n",
    "    background_optimizer.zero_grad()\n",
    "\n",
    "    output = background_model(train_data_tensor)\n",
    "    train_loss = criterion(output, train_data_tensor)\n",
    "\n",
    "    train_loss.backward()\n",
    "    background_optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Background Model Epoch [{epoch + 1}/{epochs}], Loss: {train_loss.item():.4f}')\n",
    "\n",
    "# Test data likelihood evaluation and LLR calculation\n",
    "model.eval()\n",
    "background_model.eval()\n",
    "\n",
    "# Store LLRs for normal (in-distribution) and abnormal (out-of-distribution) ECG signals\n",
    "llr_normal = []\n",
    "llr_abnormal = []\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# First half of test_data as normal signals\n",
    "for i in range(0, len(test_data)//2, batch_size):\n",
    "    test_batch = test_data[i:i+batch_size]\n",
    "    test_batch_tensor = torch.tensor(test_batch, dtype=torch.float32).unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        main_output = model(test_batch_tensor)\n",
    "        main_likelihood = criterion(main_output, test_batch_tensor).item()\n",
    "\n",
    "        background_output = background_model(test_batch_tensor)\n",
    "        background_likelihood = criterion(background_output, test_batch_tensor).item()\n",
    "\n",
    "    # Compute LLR for normal signals\n",
    "    LLR = np.log(main_likelihood) - np.log(background_likelihood)\n",
    "    llr_normal.append(LLR)\n",
    "\n",
    "# Second half of test_data as abnormal signals\n",
    "for i in range(len(test_data)//2, len(test_data), batch_size):\n",
    "    test_batch = test_data[i:i+batch_size]\n",
    "    test_batch_tensor = torch.tensor(test_batch, dtype=torch.float32).unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        main_output = model(test_batch_tensor)\n",
    "        main_likelihood = criterion(main_output, test_batch_tensor).item()\n",
    "\n",
    "        background_output = background_model(test_batch_tensor)\n",
    "        background_likelihood = criterion(background_output, test_batch_tensor).item()\n",
    "\n",
    "    # Compute LLR for abnormal signals\n",
    "    LLR = np.log(main_likelihood) - np.log(background_likelihood)\n",
    "    llr_abnormal.append(LLR)\n",
    "\n",
    "# Convert LLR lists to numpy arrays\n",
    "llr_normal = np.array(llr_normal)\n",
    "llr_abnormal = np.array(llr_abnormal)\n",
    "\n",
    "# Visualization of LLR distributions using Seaborn\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(llr_normal, color='blue', label='Normal (In-distribution)', kde=True, stat=\"density\")\n",
    "sns.histplot(llr_abnormal, color='red', label='Abnormal (Out-of-distribution)', kde=True, stat=\"density\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of Log-Likelihood Ratios (LLR)')\n",
    "plt.xlabel('Log-Likelihood Ratio (LLR)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Ground truth labels for test data\n",
    "true_labels = np.concatenate([np.zeros(len(test_data)//2), np.ones(len(test_data)//2)])\n",
    "\n",
    "# Threshold to classify based on LLR\n",
    "LLR_threshold = -1.0\n",
    "\n",
    "# Initialize lists to store predicted labels\n",
    "predicted_labels = []\n",
    "\n",
    "# Batch-wise likelihood evaluation for classification\n",
    "for i in range(0, len(test_data), batch_size):\n",
    "    test_batch = test_data[i:i+batch_size]\n",
    "    test_batch_tensor = torch.tensor(test_batch, dtype=torch.float32).unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        main_output = model(test_batch_tensor)\n",
    "        main_likelihood = criterion(main_output, test_batch_tensor).item()\n",
    "\n",
    "        background_output = background_model(test_batch_tensor)\n",
    "        background_likelihood = criterion(background_output, test_batch_tensor).item()\n",
    "\n",
    "    # Compute LLR and classify\n",
    "    LLR = np.log(main_likelihood) - np.log(background_likelihood)\n",
    "    predicted_labels += [1 if LLR > LLR_threshold else 0 for _ in test_batch]\n",
    "\n",
    "# Calculate confusion matrix and metrics\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function to compute log-likelihood\n",
    "def compute_log_likelihood(model, x_test, criterion):\n",
    "    \"\"\"\n",
    "    Computes the log likelihood of the test data given the trained model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        outputs = model(x_test)\n",
    "\n",
    "        # Compute the loss between the output and the original data\n",
    "        log_likelihood = -criterion(outputs, x_test)\n",
    "\n",
    "        # Return log-likelihood (for all test samples)\n",
    "        return log_likelihood\n",
    "\n",
    "# Define criterion (Mean Squared Error or another appropriate loss function)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Compute log-likelihood for normal (main model) and background data\n",
    "log_likelihood_normal = compute_log_likelihood(model, test_data_tensor, criterion)\n",
    "log_likelihood_background = compute_log_likelihood(background_model, test_data_tensor, criterion)\n",
    "\n",
    "# Print log-likelihood values\n",
    "print(f\"Log-Likelihood (Normal): {log_likelihood_normal.item()}\")\n",
    "print(f\"Log-Likelihood (Background): {log_likelihood_background.item()}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
